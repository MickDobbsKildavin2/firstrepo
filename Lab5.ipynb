{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343dd1e2",
   "metadata": {},
   "source": [
    "# Lab 5 for Big Data programming.\n",
    "# Apache Spark Machine Learning using Dataframes in Google Colab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16332d",
   "metadata": {},
   "source": [
    "# 1.\tSetup an Apache Spark instance in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once.\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "#Run Once\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd7c95",
   "metadata": {},
   "source": [
    "# 2.\tCreate a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Colab\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d4ca8",
   "metadata": {},
   "source": [
    "# 3.\tDownload the Iris dataset and another dataset of your choosing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" -O sample_data/iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cd4c2",
   "metadata": {},
   "source": [
    "# 4.\tImport the Iris dataset into a dataframe and insert screenshot of df.show()command output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6bcfb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a047b023670f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#df = spark.read.csv('sample_data/iris.data', header=False, sep=\",\", inferSchema=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample_data/iris.data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SepalLength\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"SepalWidth\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PetalLength\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PetalWidth\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Class\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#df = spark.read.csv('sample_data/iris.data', header=False, sep=\",\", inferSchema=True)\n",
    "df = spark.read.csv('sample_data/iris.data', inferSchema=True)\\\n",
    ".toDF(\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\",\"Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67ee40",
   "metadata": {},
   "source": [
    "# 5.\tSpark ML can only deal with one features column - so we need to vectorise the multiple columns into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9645cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler=VectorAssembler(\\\n",
    "inputCols=[\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\"],\\\n",
    "outputCol=\"features\")\n",
    "df_temp=vector_assembler.transform(df)\n",
    "df_temp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a378120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=df_temp.drop(\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\")\n",
    "df_temp.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b40bbd",
   "metadata": {},
   "source": [
    "# 6.\tThe final data preparation step is to index the Class column - to use numeric rather than text values - run the following command and display your output of Class, features & ClassIndex columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edf238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "l_indexer=StringIndexer(inputCol=\"Class\", outputCol=\"ClassIndex\")\n",
    "df = l_indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01142097",
   "metadata": {},
   "source": [
    "# 7.\tSplit your data into training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData,testData) = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccddb4c",
   "metadata": {},
   "source": [
    "# 8.\tDecision Tree Classifier \n",
    "## Specify the DecisionTreeClassifier and train the model on your training dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier \n",
    "from pyspark.ml.evaluation importMulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a360ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"ClassIndex\",featuresCol=\"features\")\n",
    "model=dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050d976",
   "metadata": {},
   "source": [
    "# 9.\tTest your model with your test dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26649045",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"prediction\",\"ClassIndex\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa3774",
   "metadata": {},
   "source": [
    "# 10.\tRun an evaluator function to show the accuracy of your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e056416",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator= MulticlassClassificationEvaluator(\\\n",
    "labelCol=\"ClassIndex\", predictionCol=\"prediction\",\\\n",
    "metricName=\"accuracy\")\n",
    "accuracy=evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test Set accuracy = \" +str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3ea17",
   "metadata": {},
   "source": [
    "# 11.\tRandom Forest Classifier\n",
    "\n",
    "## Specify the RandomForestClassifier, train the model on your training dataset, predict using your test dataset, and run an evaluator to test accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0e628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "rf=RandomForestClassifier(labelCol=\"ClassIndex\",\\\n",
    "featuresCol=\"features\",numTrees=10)\n",
    "model=rf.fit(trainingData)\n",
    "predictions=model.transform(testData)\n",
    "predictions.select(\"prediction\",\"ClassIndex\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator= \\\n",
    "MulticlassClassificationEvaluator(labelCol=\"ClassIndex\",\\\n",
    "predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy=evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test Set accuracy = \" +str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f634c3",
   "metadata": {},
   "source": [
    "# 12.\tNaive Bayes Classifier\n",
    "## Specify the NaiveBayes classifier, train the model on your training dataset, predict using your test dataset, and run an evaluator to test accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes \n",
    "nb=NaiveBayes(labelCol=\"ClassIndex\",\\\n",
    "featuresCol=\"features\",smoothing=1.0,\\\n",
    "modelType=\"multinomial\")\n",
    "model=nb.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.transform(test)\n",
    "predictions.select(\"Class\",\"ClassIndex\",\n",
    "\"probability\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator= \\\n",
    "MulticlassClassificationEvaluator(labelCol=\"ClassIndex\",\\\n",
    "predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy=evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Test Set accuracy = \" +str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9728750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
